{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0.0
    ],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Most Code here\n",
    "import sys\n",
    "import scipy.misc, scipy.ndimage.interpolation\n",
    "import pickle\n",
    "sys.path.append(\"/opt/saratan/data/layers\")\n",
    "sys.path.append(\"/opt/saratan/\")\n",
    "\n",
    "import plyvel, saratan_utils, math, re, time\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "plt.set_cmap('gray')\n",
    "%matplotlib inline\n",
    "from IPython import display\n",
    "import pandas\n",
    "from PIL import Image,ImageFilter\n",
    "\n",
    "# ADD VNET STUFF\n",
    "import VNet as VN\n",
    "import utilities\n",
    "import DataManager as DM\n",
    "import pyLayer\n",
    "import main\n",
    "\n",
    "\n",
    "def hist(arr):\n",
    "    \"\"\"Print number of pixels for each label in the given image (arr)\"\"\"\n",
    "    return \"%.3f , %.3f , %.3f, %.3f\" % (np.sum(arr==0),np.sum(arr==1),np.sum(arr==2),np.sum(arr==4))\n",
    "\n",
    "def imshow(*args,**kwargs):\n",
    "    \"\"\" Handy function to show multiple plots in on row, possibly with different cmaps and titles\n",
    "    Usage: \n",
    "    imshow(img1, title=\"myPlot\")\n",
    "    imshow(img1,img2, title=['title1','title2'])\n",
    "    imshow(img1,img2, cmap='hot')\n",
    "    imshow(img1,img2,cmap=['gray','Blues']) \"\"\"\n",
    "    cmap = kwargs.get('cmap', 'gray')\n",
    "    title= kwargs.get('title','')\n",
    "    axis_enabled = kwargs.get('axis',True)\n",
    "\n",
    "    if len(args)==0:\n",
    "        raise ValueError(\"No images given to imshow\")\n",
    "    elif len(args)==1:\n",
    "        if not axis_enabled:\n",
    "            plt.axis('off')\n",
    "        plt.title(title)\n",
    "        plt.imshow(args[0], interpolation='none')\n",
    "    else:\n",
    "        n=len(args)\n",
    "        if type(cmap)==str:\n",
    "            cmap = [cmap]*n\n",
    "        if type(title)==str:\n",
    "            title= [title]*n\n",
    "        plt.figure(figsize=(n*5,10))\n",
    "        for i in range(n):\n",
    "            plt.subplot(1,n,i+1)\n",
    "            plt.title(title[i])\n",
    "            if not axis_enabled:\n",
    "                plt.axis('off')\n",
    "            plt.imshow(args[i], cmap[i], interpolation='none')\n",
    "    plt.show()\n",
    "        \n",
    "def dice(prediction, segmentation, label_of_interest = 1):\n",
    "    \"\"\" Takes 2 2-D arrays with class labels, and return a float dice score.\n",
    "    Only label=label_of_interest is considered \"\"\"\n",
    "    if prediction.shape != segmentation.shape:\n",
    "        raise ValueError(\"Shape mismatch between given arrays. prediction %s vs segmentation %s\" \\\n",
    "                         % (str(prediction.shape), str(segmentation.shape)))\n",
    "\n",
    "    n_liver_seg = np.sum(segmentation==label_of_interest)\n",
    "    n_liver_pred= np.sum(prediction == label_of_interest)\n",
    "    denominator = n_liver_pred + n_liver_seg\n",
    "    if denominator == 0:\n",
    "        return -1\n",
    "\n",
    "    liver_intersection   = np.logical_and(prediction==label_of_interest, segmentation==label_of_interest)\n",
    "    n_liver_intersection = np.sum(liver_intersection)\n",
    "\n",
    "    dice_score = 2.0*n_liver_intersection / denominator\n",
    "    return dice_score\n",
    "\n",
    "    \n",
    "def protobinary_to_array(filename, outpng=None):\n",
    "    \"\"\" Filename is path to protobinary\n",
    "    outpng is path to output png\"\"\"\n",
    "    with open(filename,'r') as f:\n",
    "        data = f.read()\n",
    "\n",
    "    blob = caffe.proto.caffe_pb2.BlobProto()\n",
    "    blob.ParseFromString(data)\n",
    "    arr = np.array(caffe.io.blobproto_to_array(blob)) #returns shape (1,1,W,H)\n",
    "    arr = arr[0,0,:,:] #ignore first 2 dimensions\n",
    "    return  arr\n",
    "\n",
    "dices_liver = []\n",
    "dices_lesion= []\n",
    "def predict(net, img, seg, meanimg):\n",
    "    \"\"\"Predicts an img using the trained net, and compares it to the label image (seg)\"\"\"\n",
    "    net.blobs['data'].data[0]=(img-meanimg)\n",
    "    prob=net.forward()['prob'][0]\n",
    "    prediction = np.argmax(prob,axis=0)\n",
    "    dice_liver = dice(prediction,seg,label_of_interest=1)\n",
    "    dice_lesion = dice(prediction,seg,label_of_interest=2)\n",
    "    dices_liver.append(dice_liver)\n",
    "    dices_lesion.append(dice_lesion)\n",
    "    print \"Dice Liver:\", dice_liver\n",
    "    print \"Dice Lesion:\",dice_lesion\n",
    "    print \"Prediction class histogram\",hist(prediction)\n",
    "    print \"Ground truth class histogram\",hist(seg)\n",
    "    plt.figure(figsize=(20,24))\n",
    "    plt.subplot(1,3,1); plt.title(\"Image\")\n",
    "    plt.imshow(img)\n",
    "    plt.subplot(1,3,2); plt.title(\"Ground truth\")\n",
    "    plt.imshow(seg)\n",
    "    plt.subplot(1,3,3); plt.title(\"Prediction\")\n",
    "    plt.imshow(prediction)\n",
    "    plt.show()\n",
    "    \n",
    "def read_imgs(dbimgit, dbsegit, n=1, print_keys=True):\n",
    "    \"\"\"Read img and label after skipping n keys in leveldb. Takes db iterators\"\"\"\n",
    "    for _ in range(n):\n",
    "        k1,vimg = dbimgit.next()\n",
    "        k2,vseg = dbsegit.next()\n",
    "    if print_keys:\n",
    "        print \"Keys:\",k1,k2\n",
    "    img=lutils.to_numpy_matrix(vimg)\n",
    "    seg=lutils.to_numpy_matrix(vseg)\n",
    "    return img,seg\n",
    "\n",
    "def show_kernels(layer_blob_data, fast = False):\n",
    "    \"\"\" Takes solver.net.params['conv1'][0].data and visualize the first channel of all kernels.\n",
    "    If fast = False : subplots will be used, allowing to see each filter individually, but takes time.\n",
    "    If fast = True : all filters are plotted in one image\"\"\"\n",
    "    #Input has 4 dims, we only visualize 1st channel of each kernel \n",
    "    # (the conv weights that acts on the 1st channel of the input)\n",
    "    data = layer_blob_data[:,0,:,:]\n",
    "    if fast:\n",
    "        raise NotImplementedError(\"todo\")\n",
    "    \n",
    "    # Sort\n",
    "    sorted_data = sorted(data, key=lambda x: np.sum(x))\n",
    "    data = np.array(sorted_data)\n",
    "    \n",
    "    n_kernels = np.array(data).shape[0]\n",
    "    plot_cols = 20 #number of images in one row\n",
    "    plot_rows = math.ceil(n_kernels*1.0 / plot_cols)\n",
    "    # Adjust figure plot size\n",
    "    plt.figure(figsize=(min(plot_cols, n_kernels)*0.7, plot_rows*0.7))\n",
    "    # Plot !\n",
    "    vmin = np.min(data)\n",
    "    vmax = np.max(data)\n",
    "    print vmin,vmax\n",
    "    for i in range(n_kernels):\n",
    "        plt.subplot(plot_rows, plot_cols, i+1)\n",
    "        plt.imshow(data[i], interpolation='none', vmin=vmin, vmax=vmax)\n",
    "        plt.axis(\"off\")\n",
    "    plt.show()\n",
    "        \n",
    "def plot_deepliver_log(fname):\n",
    "    \"\"\"Takes file handle of deepliver log, and plots the 4 plots :\n",
    "    Loss, avgAccuracy, avgJaccard, avgRecall\"\"\"\n",
    "    f = open(fname, 'r')\n",
    "    logs = f.read()\n",
    "    plt.figure(figsize=(10,10))\n",
    "    # Get iterations\n",
    "    iterations = re.findall(\"Iteration (\\d+), loss\",logs)\n",
    "\n",
    "    # Get&plot loss\n",
    "    loss = zip(*re.findall(\"Iteration \\d+, loss = ([+-]?(\\d+(\\.\\d*)?|\\.\\d+)([eE][+-]?\\d+)?)\",logs))[0]\n",
    "    length = min(len(iterations), len(loss))\n",
    "    iterations_trunc, loss_trunc = iterations[:length], loss[:length]\n",
    "    plt.plot(iterations,loss,label='Loss')\n",
    "    #plt.show()\n",
    "    #Get&plot metrics\n",
    "    metrics = ['Accuracy','Recall','Jaccard']\n",
    "    data = defaultdict(list) # data.keys() = metrics , data[metrics[0]] = list of values\n",
    "    for i,metric in enumerate(metrics):\n",
    "        regex = \"Train net output #\"+str(i)+\": accuracy = ([+-]?(\\d+(\\.\\d*)?|\\.\\d+)([eE][+-]?\\d+)?)\"\n",
    "        for result in re.findall(regex,logs):\n",
    "            data[metric].append(result[0])\n",
    "\n",
    "    for metric in data.keys():\n",
    "        length = min(len(iterations),len(data[metric]))\n",
    "        iterations_trunc, data_trunc = iterations[:length], data[metric][:length]\n",
    "        plt.plot(iterations_trunc, data_trunc,label=metric)\n",
    "        plt.legend(loc=\"lower center\",prop={'size':15})\n",
    "    f.close()\n",
    "\n",
    "\n",
    "def histeq(im,nbr_bins=256):\n",
    "    \"\"\"Histogram equalization\"\"\"\n",
    "    #get image histogram\n",
    "    imhist,bins = np.histogram(im.flatten(),nbr_bins,normed=True)\n",
    "    cdf = imhist.cumsum() #cumulative distribution function\n",
    "    cdf = 255 * cdf / cdf[-1] #normalize\n",
    "    #use linear interpolation of cdf to find new pixel values\n",
    "    im2 = np.interp(im.flatten(),bins[:-1],cdf)\n",
    "    return im2.reshape(im.shape)\n",
    "\n",
    "def imshow_overlay_segmentation(him,img,seg,pred):\n",
    "    plt.figure(figsize=(20,10))\n",
    "    plt.subplot(1,3,1)\n",
    "    plt.title(\"Image\")\n",
    "    plt.imshow(himg)\n",
    "    plt.subplot(1,3,2)\n",
    "    plt.title(\"Ground Truth\")\n",
    "    plt.imshow(img); plt.hold(True)\n",
    "    plt.imshow(seg, cmap=\"Blues\", alpha=0.3)\n",
    "    plt.subplot(1,3,3)\n",
    "    plt.title(\"Prediction\")\n",
    "    plt.imshow(img); plt.hold(True)\n",
    "    plt.imshow(pred, cmap=\"Reds\", alpha=0.3)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import caffe\n",
    "print caffe.__file__\n",
    "caffe.set_mode_gpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load Params and NET\n",
    "\n",
    "cwd=os.getcwd()\n",
    "basePath='/media/nas/03_Users/01_patrickchrist/vnet'\n",
    "params = dict()\n",
    "params['DataManagerParams']=dict()\n",
    "params['ModelParams']=dict()\n",
    "\n",
    "#params of the algorithm\n",
    "params['ModelParams']['numcontrolpoints']=2\n",
    "params['ModelParams']['sigma']=15\n",
    "params['ModelParams']['device']=0\n",
    "params['ModelParams']['prototxtTrain']=os.path.join(cwd,'Prototxt/train_noPooling_ResNet_cinque.prototxt')\n",
    "params['ModelParams']['prototxtTest']=os.path.join(cwd,'Prototxt/test_noPooling_ResNet_cinque.prototxt')\n",
    "params['ModelParams']['snapshot']=0\n",
    "params['ModelParams']['dirTrain']='/media/nas/01_Datasets/CT/Abdomen/3Dircadb1/niftis_segmented_tumorsonly'\n",
    "params['ModelParams']['dirTest']='/media/nas/01_Datasets/CT/Abdomen/3Dircadb1/niftis_segmented_tumorsonly'\n",
    "params['ModelParams']['dirResult']=os.path.join(basePath,'results') #where we need to save the results (relative to the base path)\n",
    "params['ModelParams']['dirSnapshots']=os.path.join(basePath,'models/3dircad/') #where to save the models while training\n",
    "params['ModelParams']['batchsize'] = 2 #the batchsize\n",
    "params['ModelParams']['numIterations'] = 100000 #the number of iterations\n",
    "params['ModelParams']['baseLR'] = 0.0001 #the learning rate, initial one\n",
    "params['ModelParams']['nProc'] = 1 #the number of threads to do data augmentation\n",
    "\n",
    "\n",
    "#params of the DataManager\n",
    "params['DataManagerParams']['dstRes'] = np.asarray([1,1,1.5],dtype=float)\n",
    "params['DataManagerParams']['VolSize'] = np.asarray([128,128,64],dtype=int)\n",
    "params['DataManagerParams']['normDir'] = False #if rotates the volume according to its transformation in the mhd file. Not reccommended.\n",
    "\n",
    "model=VN.VNet(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "batchsize = params['ModelParams']['batchsize']\n",
    "\n",
    "batchData = np.zeros((batchsize, 1, params['DataManagerParams']['VolSize'][0], params['DataManagerParams']['VolSize'][1], params['DataManagerParams']['VolSize'][2]), dtype=float)\n",
    "batchLabel = np.zeros((batchsize, 1, params['DataManagerParams']['VolSize'][0], params['DataManagerParams']['VolSize'][1], params['DataManagerParams']['VolSize'][2]), dtype=float)\n",
    "\n",
    "#only used if you do weighted multinomial logistic regression\n",
    "batchWeight = np.zeros((batchsize, 1, params['DataManagerParams']['VolSize'][0],\n",
    "                       params['DataManagerParams']['VolSize'][1],\n",
    "                       params['DataManagerParams']['VolSize'][2]), dtype=float)\n",
    "\n",
    "train_loss = np.zeros(nr_iter)\n",
    "for it in range(nr_iter):\n",
    "    for i in range(batchsize):\n",
    "        [defImg, defLab, defWeight] = dataQueue.get()\n",
    "\n",
    "        batchData[i, 0, :, :, :] = defImg.astype(dtype=np.float32)\n",
    "        batchLabel[i, 0, :, :, :] = (defLab > 0.5).astype(dtype=np.float32)\n",
    "        batchWeight[i, 0, :, :, :] = defWeight.astype(dtype=np.float32)\n",
    "\n",
    "    solver.net.blobs['data'].data[...] = batchData.astype(dtype=np.float32)\n",
    "    solver.net.blobs['label'].data[...] = batchLabel.astype(dtype=np.float32)\n",
    "    #solver.net.blobs['labelWeight'].data[...] = batchWeight.astype(dtype=np.float32)\n",
    "    #use only if you do softmax with loss\n",
    "\n",
    "\n",
    "    solver.step(1)  # this does the training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "WEIGHTS_FILE= \"phseg_v5.caffemodel\"\n",
    "#WEIGHTS_FILE = \"unet_models/plainunet/fire3Best18_allslices_plainunet_wd0.001_2dropout_172.5k_0.85_0.65.caffemodel\"\n",
    "\n",
    "\n",
    "solver.net.copy_from(WEIGHTS_FILE)\n",
    "solver.test_nets[0].copy_from(WEIGHTS_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# each output is (batch size, feature dim, spatial dim)\n",
    "[(k, v.data.shape) for k, v in solver.net.blobs.items()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# just print the weight sizes (not biases)\n",
    "[(k, v[0].data.shape) for k, v in solver.net.params.items()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0.0
    ],
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TMP : Test network on 200 slices\n",
    "if False:\n",
    "    tmp_dices = []\n",
    "    neg_dice_count = 0\n",
    "    for _ in range(200):\n",
    "        solver.net.forward()\n",
    "        img_=blobs['data'].data[0,0]\n",
    "        seg_=blobs['label'].data[0,0]\n",
    "        dice_=dice(seg_,np.argmax(blobs['score'].data[0],axis=0))\n",
    "        if dice_ >= 0:\n",
    "            tmp_dices.append(dice_)\n",
    "        else:\n",
    "            neg_dice_count += 1\n",
    "        #print \"Dice\", tmp_dices[-1]\n",
    "        #imshow(blobs['data'].data[0,0], blobs['label'].data[0,0], np.argmax(blobs['score'].data[0],axis=0))\n",
    "        n_liver=np.sum(seg_>0)\n",
    "        percent_liver = 100.0*n_liver / seg_.size\n",
    "        #print \"Liver is\",percent_liver,\"% of the image\"\n",
    "    print \"Avg dice\",np.average(tmp_dices)\n",
    "    print \"-1's :\",neg_dice_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "solver.net.forward()\n",
    "print 'dice 1', dice(blobs['label'].data[0,0], np.argmax(blobs['score'].data[0],axis=0), label_of_interest=1)\n",
    "print 'dice 2', dice(blobs['label'].data[0,0], np.argmax(blobs['score'].data[0],axis=0), label_of_interest=2)\n",
    "imshow(blobs['data'].data[0,0], blobs['label'].data[0,0], np.argmax(blobs['score'].data[0],axis=0), axis=False,title=[\"Slice\",\"Ground truth\",\"Prediction\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choose below if you want to enable monitoring a third label : set enable_label_2 = True ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0.0
    ],
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Config and Initialization\n",
    "enable_label_2 = True #Set to true when segmenting both liver and lesion (labels=0,1,2)\n",
    "use_label1_redblue = False # use redblue dice plot. Useful when training cascade Step2 \n",
    "LOAD_ARRAYS = True # Load arrays from pickled files\n",
    "\n",
    "if use_label1_redblue:\n",
    "    label1_color_train, label1_color_test = \"blue\", \"red\" \n",
    "else:\n",
    "    label1_color_train, label1_color_test = \"#ADB317\", \"#1C7A34\"\n",
    "\n",
    "\n",
    "PLOT_INTERVAL = 100 # Plot one data point every n iterations\n",
    "dices = [] #dices for label=1\n",
    "dices_2 = [] #dices for label=2\n",
    "losses= []\n",
    "accuracies=[]\n",
    "iterations=[]\n",
    "test_dices=[]\n",
    "test_dices_2=[]\n",
    "test_accuracies=[]\n",
    "i = 0\n",
    "if LOAD_ARRAYS:\n",
    "    i=                pickle.load(open(\"i.int\",'r'))\n",
    "    dices=            pickle.load(open(\"dices.list\",'r'))\n",
    "    if enable_label_2:\n",
    "        dices_2=          pickle.load(open(\"dices_2.list\",'r'))\n",
    "        test_dices_2 =    pickle.load(open(\"test_dices_2.list\",'r'))\n",
    "    losses=           pickle.load(open(\"losses.list\",'r'))\n",
    "    accuracies=       pickle.load(open(\"accuracies.list\",'r'))\n",
    "    iterations =      pickle.load(open(\"iterations.list\",'r'))\n",
    "    test_dices =      pickle.load(open(\"test_dices.list\",'r'))\n",
    "    test_accuracies = pickle.load(open(\"test_accuracies.list\",'r'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To resume run this ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print len(iterations),len(dices),len(dices_2),len(losses),len(accuracies),len(iterations),len(test_dices),len(test_dices_2),len(test_accuracies)\n",
    "if not enable_label_2:\n",
    "    test_dices_2 = test_dices\n",
    "    dices_2 = dices\n",
    "min_len=min(len(iterations),len(dices),len(dices_2),len(losses),len(accuracies),len(iterations),len(test_dices),len(test_dices_2),len(test_accuracies))\n",
    "print \"Min len\",min_len\n",
    "print 'i',i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### then this ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# In case of resumed training, make sure all lists have equal size. Since kernel interruption might cause them to be \n",
    "# not equal\n",
    "#n_ignored_entries = min_len%PLOT_INTERVAL\n",
    "#min_len -= n_ignored_entries\n",
    "if len(dices) % 100 != 0 and len(dices) > len(test_dices):\n",
    "    n_ignored_entries = len(dices) - len(test_dices)\n",
    "    min_len = len(dices) - n_ignored_entries\n",
    "    dices = dices[:min_len]\n",
    "    dices_2=dices_2[:min_len]\n",
    "    losses= losses[:min_len]\n",
    "    accuracies=accuracies[:min_len]\n",
    "    iterations=iterations[:min_len]\n",
    "    test_dices=test_dices[:min_len]\n",
    "    test_dices_2=test_dices_2[:min_len]\n",
    "    test_accuracies=test_accuracies[:min_len]\n",
    "    i = len(dices) * PLOT_INTERVAL\n",
    "\n",
    "print len(iterations),len(dices),len(dices_2),len(losses),len(accuracies),len(iterations),len(test_dices),len(test_accuracies)\n",
    "print 'i',i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### You can stop the below training cell - then run the above 2 cells before resuming ####\n",
    "# TRAIN here #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0.0
    ],
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TRAIN\n",
    "\n",
    "def smooth_last_n(arr, n=5, ignore=None):\n",
    "    \"\"\"Replaces the last n elements in arr (list) with their average.\"\"\"\n",
    "    subarr = np.array(arr[-n:])\n",
    "    if ignore != None:\n",
    "        subarr = subarr[subarr != ignore] \n",
    "    mean = np.mean(subarr)\n",
    "    return arr[:-n]+[mean]\n",
    "\n",
    "iteration_times = []\n",
    "while True:\n",
    "    i += 1\n",
    "    start_ts = time.time()\n",
    "    solver.step(1)\n",
    "    end_ts   = time.time()\n",
    "    iteration_times.append(end_ts-start_ts)\n",
    "\n",
    "    # Get metrics\n",
    "    img = blobs['data'].data[0,0]\n",
    "    seg = blobs['label'].data[0,0]\n",
    "    pred= np.argmax(blobs['score'].data[0],axis=0)\n",
    "    dice_score = dice(pred,seg,1)\n",
    "    dice_score_2 = dice(pred,seg,2) if enable_label_2 else 0\n",
    "    accuracy_score = np.sum(seg==pred)*1.0 / seg.size\n",
    "    loss = float(solver.net.blobs['loss'].data)\n",
    "    \n",
    "    #Save metrics values\n",
    "    iterations.append(i)\n",
    "    dices.append(dice_score if dice_score>-1 else 1)\n",
    "    dices_2.append(dice_score_2 if dice_score_2>-1 else 1)\n",
    "    accuracies.append(accuracy_score)\n",
    "    losses.append(loss)\n",
    "    \n",
    "    if i % PLOT_INTERVAL == 0:\n",
    "        display.clear_output(wait=True)\n",
    "        \n",
    "        # Print timing stats\n",
    "        avg_iteration_time = np.mean(iteration_times)\n",
    "        iteration_times = []\n",
    "        \n",
    "        liver_train_dices = []\n",
    "        for _ in range(PLOT_INTERVAL):\n",
    "            solver.test_nets[0].forward()\n",
    "            test_img = testblobs['data'].data[0,0]\n",
    "            test_seg = testblobs['label'].data[0,0]\n",
    "            test_pred= np.argmax(testblobs['score'].data[0], axis=0)\n",
    "\n",
    "            test_dice_score = dice(test_pred, test_seg, 1)\n",
    "            test_dice_score_2 = dice(test_pred, test_seg, 2) if enable_label_2 else 0\n",
    "            test_accuracy_score = np.sum(test_seg==test_pred)*1.0 / test_seg.size\n",
    "\n",
    "            test_dices.append(test_dice_score if test_dice_score > -1 else 1)\n",
    "            test_dices_2.append(test_dice_score_2 if test_dice_score_2 > -1 else 1)\n",
    "            test_accuracies.append(test_accuracy_score)\n",
    "            \n",
    "        \n",
    "        \n",
    "        # Smooth\n",
    "        iterations = smooth_last_n(iterations  ,n=PLOT_INTERVAL)\n",
    "        losses     = smooth_last_n(losses      ,n=PLOT_INTERVAL)\n",
    "        dices      = smooth_last_n(dices       ,n=PLOT_INTERVAL)\n",
    "        dices_2    = smooth_last_n(dices_2     ,n=PLOT_INTERVAL) if enable_label_2 else []\n",
    "        accuracies = smooth_last_n(accuracies  ,n=PLOT_INTERVAL)\n",
    "        test_dices = smooth_last_n(test_dices  ,n=PLOT_INTERVAL)\n",
    "        test_dices_2=smooth_last_n(test_dices_2,n=PLOT_INTERVAL) if enable_label_2 else []\n",
    "        test_accuracies = smooth_last_n(test_accuracies,n=PLOT_INTERVAL)\n",
    "        \n",
    "        # Print last metrics\n",
    "        print \"Average solver.step duration is\", avg_iteration_time\n",
    "        print 'Loss',losses[-1]\n",
    "        print '#### ACCURACY ####'\n",
    "        print 'Train Accuracy', accuracies[-1]\n",
    "        print 'Test Accuracy', test_accuracies[-1]\n",
    "        print \"#### DICE ####\"\n",
    "        print 'Train dice (label=1)',dices[-1]\n",
    "        print 'Test dice (label=1)', test_dices[-1]\n",
    "        if enable_label_2:\n",
    "            print 'Train dice (label=2)',dices_2[-1]\n",
    "            print 'Test dice (label=2)', test_dices_2[-1]\n",
    "        print '\\n'\n",
    "        \n",
    "        # Plot\n",
    "        fig, ax1=plt.subplots()\n",
    "        ax2=ax1.twinx()\n",
    "        ax1.set_xlabel(\"Iterations\")\n",
    "        ax2.set_ylabel(\"Dice\")\n",
    "        ax1.set_ylabel(\"Loss\")\n",
    "        ax2.plot(iterations, dices, label=\"Train Dice - Label=1\", color=label1_color_train); plt.hold(True) #dark yellow\n",
    "        ax2.plot(iterations, test_dices, label=\"Test Dice - Label=1\", color=label1_color_test); plt.hold(True) #green\n",
    "        ax1.plot(iterations, losses, label=\"Loss\", color=\"black\"); plt.hold(True)\n",
    "        leg1 = ax2.legend(loc=\"upper left\", bbox_to_anchor=(1.15,1))\n",
    "        leg2 = ax1.legend(loc=\"upper left\", bbox_to_anchor=(1.15,0.5))\n",
    "        # Make legend clearer\n",
    "        for leghandle in leg1.legendHandles+leg2.legendHandles: leghandle.set_linewidth(10.0)\n",
    "        plt.show()\n",
    "        \n",
    "        \n",
    "        if enable_label_2:\n",
    "            fig, ax1=plt.subplots()\n",
    "            ax2=ax1.twinx()\n",
    "            ax1.set_xlabel(\"Iterations\")\n",
    "            ax2.set_ylabel(\"Dice\")\n",
    "            ax1.set_ylabel(\"Loss\")\n",
    "            ax2.plot(iterations, dices_2, label=\"Train Dice - Label=2\",color=\"blue\"); plt.hold(True) #purple #BC23C4\n",
    "            ax2.plot(iterations, test_dices_2, label=\"Test Dice - Label=2\",color=\"red\"); plt.hold(True) #red\n",
    "            ax1.plot(iterations, losses, label=\"Loss\", color=\"black\"); plt.hold(True)\n",
    "            leg1 = ax2.legend(loc=\"upper left\", bbox_to_anchor=(1.15,1))\n",
    "            leg2 = ax1.legend(loc=\"upper left\", bbox_to_anchor=(1.15,0.5))\n",
    "            # Make legend clearer\n",
    "            for leghandle in leg1.legendHandles+leg2.legendHandles: leghandle.set_linewidth(10.0)\n",
    "            plt.show()\n",
    "\n",
    "        print \"Iteration:\", i\n",
    "        print 'Train accuracy on last image :', np.sum(pred==seg)*1.0/pred.size\n",
    "        print 'Train dice Label=1 on last image : ', dice(pred,seg,1)\n",
    "        if enable_label_2:\n",
    "            print 'Train dice Label=2 on last image : ', dice(pred,seg,2)\n",
    "        imshow(img, seg, pred, title=[\"Train Image\", \"Ground truth\", \"Prediction\"])\n",
    "        print 'Test accuracy on last image :', np.sum(test_pred==test_seg)*1.0/test_pred.size\n",
    "        print 'Test dice Label=1 on last image : ', dice(test_pred,test_seg,1)\n",
    "        if enable_label_2:\n",
    "            print 'Test dice Label=2 on last image : ', dice(test_pred,test_seg,2)\n",
    "        imshow(test_img, test_seg, test_pred, title=[\"Test Image\", \"Ground truth\", \"Prediction\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## ---- End of training notebook (the rest is one-off analysis) ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "td2pure = td2[np.logical_and(td2!=1.0, td2!=0.0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.count_nonzero(test_dices_2==1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fidx=1\n",
    "layer_name='u0c'\n",
    "for fidx in range(blobs[layer_name].data.shape[1]):\n",
    "    last_layer_img = blobs[layer_name].data[0,fidx, :,:]\n",
    "    imshow(last_layer_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i in iterations:\n",
    "    print i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lyr=solver.net.layers[64]\n",
    "lyr.type\n",
    "lyr.blobs.__len__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for param_name in solver.net.params:\n",
    "    print param_name,\"\\t\",solver.net.params[param_name][0].data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "show_kernels(solver.net.params[\"conv_d0a-b\"][0].data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for idx in range(blobs['d2c'].data.shape[1]):\n",
    "    imshow(blobs['d2c'].data[0,idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "show_kernels(blobs['d0b'].data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "blobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print solver.net.params[\"conv_d0a-b\"][0].data.shape\n",
    "imshow(solver.net.params[\"conv_d0a-b\"][0].data[7,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "imshow(blobs['data'].data[0,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from scipy.signal import convolve2d\n",
    "filter_idx = 7\n",
    "image = blobs['data'].data[0,0]\n",
    "bias = solver.net.params[\"conv_d0a-b\"][1].data[filter_idx]\n",
    "kernel = solver.net.params[\"conv_d0a-b\"][0].data[filter_idx,0]\n",
    "result = convolve2d(image, kernel) + bias\n",
    "print bias\n",
    "imshow(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import scipy.signal.convolve2d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print blobs['d0b'].data.shape\n",
    "imshow(blobs['d0b'].data[0,15],blobs['d0b'].data[0,7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get iteration with best test dice ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dice_iter = zip(test_dices,iterations)\n",
    "dice_iter = sorted(dice_iter, key=lambda t:t[0], reverse=True)\n",
    "for ji in range(10):\n",
    "    print str(ji+1)+'th best test Dice:\\t',round(dice_iter[ji][0],3),'\\tAt iteration:\\t',dice_iter[ji][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Save plots\n",
    "import pickle\n",
    "pickle.dump(i, open(\"i.int\",'w'))\n",
    "pickle.dump(dices, open(\"dices.list\",'w'))\n",
    "pickle.dump(dices_2, open(\"dices_2.list\",'w'))\n",
    "pickle.dump(losses, open(\"losses.list\",'w'))\n",
    "pickle.dump(accuracies, open(\"accuracies.list\",'w'))\n",
    "pickle.dump(iterations, open(\"iterations.list\",'w'))\n",
    "pickle.dump(test_dices, open(\"test_dices.list\",'w'))\n",
    "pickle.dump(test_dices_2, open(\"test_dices_2.list\",'w'))\n",
    "pickle.dump(test_accuracies, open(\"test_accuracies.list\",'w'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mean = protobinary_to_array(\"mean.protobinary\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting Training examples ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for _ in range(20):\n",
    "    solver.net.forward()\n",
    "    img = (blobs['data'].data[0,0]+mean)[92:480,92:480]\n",
    "    himg =histeq(img)\n",
    "    seg = blobs['label'].data[0,0]\n",
    "    pred= np.argmax(blobs['score'].data[0],axis=0)\n",
    "\n",
    "    imshow_overlay_segmentation(himg,img,seg,pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Predict TEST examples #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for _ in range(20):\n",
    "    solver.test_nets[0].forward()\n",
    "    img = (testblobs['data'].data[0,0]+mean)[92:480,92:480]\n",
    "    himg =histeq(img)\n",
    "    seg = testblobs['label'].data[0,0]\n",
    "    pred= np.argmax(testblobs['score'].data[0],axis=0)\n",
    "\n",
    "    imshow_overlay_segmentation(himg,img,seg,pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Avg Dice score over slices #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#dices_lesions_ = []\n",
    "for _ in range(1000):\n",
    "    solver.test_nets[0].forward()\n",
    "    seg = solver.test_nets[0].blobs['label'].data[0,0]\n",
    "    pred = solver.test_nets[0].blobs['score'].data[0].argmax(0)\n",
    "    dice_lesion_ = dice(pred,seg,label_of_interest=1)\n",
    "\n",
    "    if(dice_lesion_ > -1):\n",
    "        dices_lesions_.append(dice_lesion_)\n",
    "    print \"Average TEST dice lesion: \", np.average(dices_lesions_)\n",
    "\n",
    "print \"FINAL Average TEST dice lesion: \", np.average(dices_lesions_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Avg Dice score over individual Lesions #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import scipy.spatial.distance\n",
    "import scipy.ndimage\n",
    "import scipy.ndimage.measurements\n",
    "from collections import defaultdict\n",
    "def dice_separate_lesions(seg,pred, plot=False):\n",
    "    \"\"\"Returns Avg dice of lesion structures and weight to assign to this avg dice.\"\"\"\n",
    "    #Ignore liver\n",
    "    if np.unique(seg).size > 2:\n",
    "        seg[seg==1] = 0\n",
    "        seg[seg==2] = 1\n",
    "    if np.unique(pred).size > 2:\n",
    "        pred[pred==1] = 0\n",
    "        pred[pred==2] = 1\n",
    "    # First component is always background\n",
    "    seg[0,0] = 0\n",
    "    pred[0,0] = 0\n",
    "    # Get connected components\n",
    "    comps_seg, num_comps_seg = scipy.ndimage.label(seg)\n",
    "    comps_pred, num_comps_pred = scipy.ndimage.label(pred)\n",
    "    #print 'Found n connected components in ground truth (not including bg) :', num_comps_seg\n",
    "    if plot: imshow(comps_seg, comps_pred, cmap=\"Spectral\", title=['Components in Ground Truth','Components in Prediction'])\n",
    "    # Get component centroids\n",
    "    centroids_seg = scipy.ndimage.measurements.center_of_mass(seg, comps_seg, range(1, num_comps_seg+1))\n",
    "    centroids_pred = scipy.ndimage.measurements.center_of_mass(pred, comps_pred, range(1, num_comps_pred+1))\n",
    "    # round to nearest 2 decimals (otherwise we might have problems removing from list by-value due to fp inaccuracies)\n",
    "    centroids_seg = map(lambda t:(round(t[0],2), round(t[1],2)), centroids_seg)\n",
    "    centroids_pred = map(lambda t:(round(t[0],2), round(t[1],2)), centroids_pred)\n",
    "    \n",
    "    def plot_centroids(comps_img, centroids, title, w=5):\n",
    "        centroid_img = np.ones(comps_img.shape)\n",
    "        for x,y in centroids:\n",
    "            centroid_img[x-w:x+w, y-w:y+w] = 0\n",
    "        plt.title(title)\n",
    "        plt.imshow(comps_img, cmap=\"Spectral\"); plt.hold(True)\n",
    "        plt.imshow(centroid_img,cmap=\"Reds\",alpha=0.5)\n",
    "        plt.show()\n",
    "    \n",
    "    \n",
    "    if plot: plot_centroids(comps_seg, centroids_seg, \"Centroids in Ground Truth\")\n",
    "    if plot: plot_centroids(comps_pred, centroids_pred, \"Centroids in Prediction\")\n",
    "    \n",
    "    #### Get Average dice ####\n",
    "    def get_closest(xy, list_xy, except_at_idx):\n",
    "        \"\"\"Returns the index of coordinate in list_xy that is closest to xy (euclidean distance)\n",
    "        example: get_closest((100,100), [(3,4), (5,9), (101,102), (9999,9999)]) = 2\n",
    "        because (101,102) is the closest to (100,100).\n",
    "        except_at_idx is a list of coordinate indices to ignore in list_xy\"\"\"\n",
    "        closest_idx = -1\n",
    "        min_dist = sys.maxint\n",
    "        for i, xy_dest in enumerate(list_xy):\n",
    "            if i in except_at_idx:\n",
    "                continue\n",
    "            dist = scipy.spatial.distance.euclidean(xy, xy_dest)\n",
    "            if dist < min_dist:\n",
    "                closest_idx = i\n",
    "                min_dist = dist\n",
    "        #print xy, map(lambda t:(round(t[0]),round(t[1])),list_xy), closest_idx\n",
    "        return closest_idx\n",
    "    \n",
    "    dices = []\n",
    "    consumed_lesions_idx = [] #indices of lesions already consumed.\n",
    "    # Iterate after bg component\n",
    "    for i in range(num_comps_pred):\n",
    "        # Add 0 dice to false positives!\n",
    "        if len(centroids_seg) == 0:\n",
    "            dices.append(0)\n",
    "            continue\n",
    "        current_xy = centroids_pred[i]\n",
    "        closest_component = get_closest(current_xy, centroids_seg, except_at_idx=consumed_lesions_idx)\n",
    "        consumed_lesions_idx.append(closest_component)\n",
    "        #mask out other components \n",
    "        one_lesion_pred = np.clip(comps_pred == i, 0, 1)\n",
    "        one_lesion_seg = np.clip(comps_seg == closest_component, 0, 1)\n",
    "        dices.append(dice(one_lesion_pred, one_lesion_seg, label_of_interest = 1))\n",
    "    \n",
    "    # Add 0 dice for false negatives\n",
    "    if len(centroids_seg)-len(consumed_lesions_idx) > 0:\n",
    "        dices.extend([0]*(len(centroids_seg)-len(consumed_lesions_idx)))\n",
    "\n",
    "    return np.mean(dices), len(dices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dices_lesions_ = []\n",
    "weights = []\n",
    "for _ in range(200):\n",
    "    solver.test_nets[0].forward()\n",
    "    seg = solver.test_nets[0].blobs['label'].data[0,0]\n",
    "    pred = solver.test_nets[0].blobs['score'].data[0].argmax(0)\n",
    "    dice_lesion_,weight = dice_separate_lesions(seg,pred)\n",
    "\n",
    "    if(dice_lesion_ > -1):\n",
    "        dices_lesions_.append(dice_lesion_)\n",
    "        weights.append(weight)\n",
    "\n",
    "total = np.multiply(dices_lesions_, weights)\n",
    "print \"Average TEST dice lesion: \", np.average(total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Changing threshold (instead of 0.5) ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "solver.test_nets[0].forward()\n",
    "img = (testblobs['data'].data[0,0]+mean)[92:480,92:480]\n",
    "himg =histeq(img)\n",
    "seg = testblobs['label'].data[0,0]\n",
    "pred= np.argmax(testblobs['score'].data[0],axis=0)\n",
    "imshow_overlay_segmentation(himg,img,seg,pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "prob = testblobs['prob'].data[0,1]\n",
    "imshow(prob)\n",
    "pred_t = prob>0.7\n",
    "print dice(seg,pred)\n",
    "print dice(seg,pred_t)\n",
    "imshow(pred_t,seg)\n",
    "#prob[np.logical_and(prob>0.4, prob<0.6)].size*1.0/prob.size\n",
    "\n",
    "\n",
    "#imshow_overlay_segmentation(himg,img,seg,pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def softmax(a1,a2):\n",
    "    s1 = np.exp(a1)\n",
    "    s2 = np.exp(a2)\n",
    "    sm= s1+s2\n",
    "    return s1/sm , s2/sm\n",
    "\n",
    "thresholds = np.linspace(0,1,40) #20 thresholds\n",
    "dices_athalf = []\n",
    "dices_bythreshold = defaultdict(list) # {0.5:[list of dices], 0.6:[list of dices]}\n",
    "for _ in range(2000):\n",
    "    solver.net.forward()\n",
    "    img = (blobs['data'].data[0,0]+mean)[92:480,92:480]\n",
    "    himg =histeq(img)\n",
    "    seg = blobs['label'].data[0,0]\n",
    "    prob = softmax(blobs['score'].data[0,0],blobs['score'].data[0,1])[1] #probability being a lesion\n",
    "    dices_athalf.append(dice(seg,prob>0.5))\n",
    "    for t in thresholds :\n",
    "        pred_t = prob > t\n",
    "        dice_score = dice(seg,pred_t)\n",
    "        dices_bythreshold[t].append(dice_score)\n",
    "        \n",
    "# Aggregate dices over slices for each threshold\n",
    "avgdices = []\n",
    "for t in thresholds:\n",
    "    avgdices.append(np.average(dices_bythreshold[t]))\n",
    "\n",
    "plt.plot(thresholds,avgdices)\n",
    "print 'Found max dice at threshold :', thresholds[np.argmax(avgdices)]\n",
    "print 'Found max dice score :', np.max(avgdices)\n",
    "print 'Vs. the dice at 0.5 which equals :', np.average(dices_athalf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "THRESHOLD = 0.820512820513\n",
    "dices = []\n",
    "for _ in range(2000):\n",
    "    solver.test_nets[0].forward()\n",
    "    img = (testblobs['data'].data[0,0]+mean)[92:480,92:480]\n",
    "    seg = testblobs['label'].data[0,0]\n",
    "    prob = testblobs['prob'].data[0,1] #probability being a lesion\n",
    "    dice_score = dice(seg,prob> THRESHOLD)\n",
    "    dices.append(dice_score)\n",
    "    \n",
    "print 'Average TEST threshold :', np.average(dices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dices = []\n",
    "for _ in range(2000):\n",
    "    solver.test_nets[0].forward()\n",
    "    img = (testblobs['data'].data[0,0]+mean)[92:480,92:480]\n",
    "    seg = testblobs['label'].data[0,0]\n",
    "    pred = np.argmax(testblobs['prob'].data[0], axis=0) #probability being a lesion\n",
    "    dice_score = dice(seg,pred)\n",
    "    dices.append(dice_score)\n",
    "    \n",
    "print 'Average TEST threshold :', np.average(dices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "thresholds = np.linspace(0,1,40) #20 thresholds\n",
    "dices_athalf = []\n",
    "dices_bythreshold = defaultdict(list) # {0.5:[list of dices], 0.6:[list of dices]}\n",
    "for _ in range(2000):\n",
    "    solver.test_nets[0].forward()\n",
    "    img = (testblobs['data'].data[0,0]+mean)[92:480,92:480]\n",
    "    himg =histeq(img)\n",
    "    seg = testblobs['label'].data[0,0]\n",
    "    prob = testblobs['prob'].data[0,1] #probability being a lesion\n",
    "    dices_athalf.append(dice(seg,prob>0.5))\n",
    "    for t in thresholds :\n",
    "        pred_t = prob > t\n",
    "        dice_score = dice(seg,pred_t)\n",
    "        dices_bythreshold[t].append(dice_score)\n",
    "        \n",
    "# Aggregate dices over slices for each threshold\n",
    "avgdices = []\n",
    "for t in thresholds:\n",
    "    avgdices.append(np.average(dices_bythreshold[t]))\n",
    "\n",
    "plt.plot(thresholds,avgdices)\n",
    "print 'Found max dice at threshold :', thresholds[np.argmax(avgdices)]\n",
    "print 'Found max dice score :', np.max(avgdices)\n",
    "print 'Vs. the dice at 0.5 which equals :', np.average(dices_athalf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prototxts #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!cat solver_unet.prototxt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!cat unet-overfit.prototxt"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2.0
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}